import os
import re
import numpy as np
import pandas as pd
from sklearn import metrics
from custom_knn_classifier import CustomKNN
from custom_naive_bayes import CustomNaiveBayes
from sklearn_svm_classifier import SVMClassifier
from sklearn.utils import shuffle
from nltk.corpus import stopwords
import sys


##################################################################
####################### DATASET FUNCTIONS ########################
##################################################################

def read_dataset(folder, split):
    print('***** Reading the dataset *****')
    print(os.path.join(folder, f'{split}.tsv'))

    inputf = open(os.path.join(folder, f'{folder}_{split}.tsv'),
                  encoding='utf-8')
    inputdf = pd.read_csv(inputf, sep="\t", encoding='utf-8', header=0)

    texts = inputdf.tweet_text.to_list()
    labels = inputdf.class_label.to_list()

    assert len(texts) == len(labels), ('Text and label files should'
                                       'have same number of lines..')
    print(f'Number of samples: {len(texts)}')

    return texts, labels


def preprocess_dataset(text_list):
    """
    Return the list of sentences after preprocessing. Example:
    >>> preprocess_dataset(['the quick brown fox #HASTAG-1234 @USER-XYZ'])
    ['the quick brown fox']
    """
    '''
    # all this is from
    https://www.kaggle.com/code/redwankarimsony/nlp-101-tweet-sentiment-analysis-preprocessing
    '''

    preprocessed_text_list = [
        re.sub(r'^RT[\s]+|https?:\/\/.*[\r\n]*|#|@[A-Za-z0-9_]+|[0-9]|[^\w\s]',
               '', tweet).lower() for tweet in text_list]
    # https misschien weghalen want dan gaat accuracy omhoog

    return preprocessed_text_list

##################################################################
####################### EVALUATION METRICS #######################
##################################################################


def evaluate(true_labels, predicted_labels):
    """
    Print accuracy, precision, recall and f1 metrics for each classes and
    macro average.
    >>> evaluate(true_labels=[1,0,3,2,0], predicted_labels=[1,3,2,2,0])
    accuracy: 0.6
    precision: [1. , 1. , 0.5, 0. ]
    recall: [0.5, 1. , 1. , 0. ]
    f1: [0.66666667, 1. , 0.66666667, 0.]

    macro avg:
    precision: 0.625
    recall: 0.625
    f1: 0.583
    """

    accuracy = metrics.accuracy_score(true_labels, predicted_labels)
    print(f'accuracy: {accuracy}')

    precision = metrics.precision_score(true_labels, predicted_labels,
                                        average=None)
    print(f'precision: {precision}')

    recall = metrics.recall_score(true_labels, predicted_labels, average=None)
    print(f'recall: {recall}')

    f1 = metrics.f1_score(true_labels, predicted_labels, average=None)
    print(f'f1: {f1}\n')

    precision = metrics.precision_score(true_labels, predicted_labels,
                                        average='macro')
    print(f'macro avg:\nprecision: {precision}')

    recall = metrics.recall_score(true_labels, predicted_labels,
                                  average='macro')
    print(f'recall: {recall}')

    f1 = metrics.f1_score(true_labels, predicted_labels, average='macro')
    print(f'f1: {f1}')

    # Use confusion matrix generated by the provided function
    # to calculate evaluation metrics
    confusion_matrix = metrics.confusion_matrix(y_true=true_labels,
                                                y_pred=predicted_labels)
    print(confusion_matrix)

    print('***** Evaluation *****')

    return f1


def train_test(train_data, train_labels, test_data, test_labels,
               classifier='svm'):

    # Create a your custom classifier
    if classifier == 'svm':
        cls = SVMClassifier(kernel='linear')
    elif classifier == 'naive_bayes':
        cls = CustomNaiveBayes()
    elif classifier == 'knn':
        cls = CustomKNN(k=5, distance_metric='cosine')

    # Generate features from train and test data
    # features: word count features per sentences as a 2D numpy array
    train_feats = cls.get_features(train_data)
    # train_feats = cls.tf_idf(train_feats)
    test_feats = cls.get_features(test_data)
    # test_feats = cls.tf_idf(test_feats)

    # Train classifier
    cls.fit(train_feats, train_labels)

    # Predict labels for test data by using trained classifier
    # and features of the test data
    predicted_test_labels = cls.predict(test_feats)

    # Evaluate the classifier by comparing predicted test labels
    # and true test labels
    return evaluate(test_labels, predicted_test_labels)


def cross_validate(train_data, train_labels, n_fold=10, classifier='svm'):
    """
    Implement N-fold (n_fold) cross-validation by randomly splitting
    taining data/features into N-fold Store f1-mesure scores in a list
    for result of each fold and return this list
    Check main() for using required functions
    >>> cross_validate(n_fold=3, classifier='svm')
    [0.5, 0.4, 0.6]
    """

    # Shuffle train data and tran labels with the same indexes (random_state
    # for reproducing same shuffling)
    train_data, train_labels = shuffle(train_data, train_labels,
                                       random_state=0)

    # Split training data and labels into N folds
    lenfolds = len(train_data)/n_fold

    scores = []

    for i in range(n_fold):
        test_data = train_data[int(i*lenfolds):int((i+1)*lenfolds)]
        test_labels = train_labels[int(i*lenfolds):int((i+1)*lenfolds)]

        new_train_data = train_data.copy()
        new_train_labels = train_labels.copy()
        new_train_data = [new_train_data[j] for j in range(len(new_train_data))
                          if j < (i*lenfolds) or j >= ((i+1)*lenfolds)]
        new_train_labels = [new_train_labels[j] for j in
                            range(len(new_train_labels)) if j < (i*lenfolds) or
                            j >= ((i+1)*lenfolds)]

        scores.append(train_test(new_train_data, new_train_labels, test_data,
                                 test_labels, sys.argv[1]))

    print(f'Average [evaluation measures] for {n_fold}-fold:'
          f'{np.mean(np.array(scores))}')

    return np.mean(np.array(scores))


def main():
    # Read train and test data and generate tweet list together with label list
    train_data, train_labels = read_dataset('CT22_dutch_1B_claim', 'train')
    test_data, test_labels = read_dataset('CT22_dutch_1B_claim', 'dev_test')

    train_data = preprocess_dataset(train_data)
    test_data = preprocess_dataset(test_data)

    if len(sys.argv) == 2:
        train_test(train_data, train_labels, test_data, test_labels,
                   sys.argv[1])
    elif sys.argv[2] == 'cross_validate':
        cross_validate(train_data, train_labels)
    else:
        print('Error. Usage: assignemnt_classification.py'
              ' {classifier} {cross_validate}')


if __name__ == "__main__":
    main()
